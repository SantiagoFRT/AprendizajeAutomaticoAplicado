############# Cluster Analysis in Python ###############
##### February, 2023 #############


############################# 1. Introduction to Clustering #####################################


# Unsupervised learning in real world
# Which of the following examples can be solved with unsupervised learning?


[ ]A list of tweets to be classified based on their sentiment, the data has tweets associated with a positive or negative sentiment.


[ ]A spam recognition system that marks incoming emails as spam, the data has emails marked as spam and not spam.


[x]Segmentation of learners at DataCamp based on courses they complete. The training data has no labels.


That is correct! As the training data has no labels, 
an unsupervised algorithm needs to be used 
to understand patterns in the data.


########### Pokémon sightings ############

# Import plotting class from matplotlib library
from matplotlib import pyplot as plt

# Create a scatter plot
plt.scatter(x, y)

# Display the scatter plot
plt.show()

That is correct! Notice the areas where the sightings are dense. 
This indicates that there is not one, but two legendary Pokémon out there!



################ Pokémon sightings: hierarchical clustering ################

# Import linkage and fcluster functions
from scipy.cluster.hierarchy import linkage, fcluster

# Use the linkage() function to compute distance
Z = linkage(df, 'ward')

# Generate cluster labels
df['cluster_labels'] = fcluster(Z, 2, criterion='maxclust')

# Plot the points with seaborn
sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df)
plt.show()

You are correct! Notice that the cluster labels are plotted with different colors.




############## Pokémon sightings: k-means clustering ###################

# Import kmeans and vq functions
from scipy.cluster.vq import kmeans, vq

# Compute cluster centers
centroids,_ = kmeans(df, 2)

# Assign cluster labels
df['cluster_labels'], _ = vq(df, centroids)

# Plot the points with seaborn
sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df)
plt.show()

You are correct! Notice that in this case, 
the results of both types of clustering are similar.
We will look at distinctly different results later in the course.




###################### Normalize basic list data ################

# Import the whiten function
from scipy.cluster.vq import whiten

goals_for = [4,3,2,3,1,1,2,0,1,4]

# Use the whiten() function to standardize the data
scaled_data = whiten(goals_for)
print(scaled_data)

That is right! Notice the scaled values have less variations in them.
You will now visualize the data in the next exercise.



################ Visualize normalized data ###############

# Plot original data
plt.plot(goals_for, label='original')

# Plot scaled data
plt.plot(scaled_data, label='scaled')

# Show the legend in the plot
plt.legend()

# Display the plot
plt.show()


That is right! Notice the scaled values have lower variations in them.



################# Normalization of small numbers ##################

# Prepare data
rate_cuts = [0.0025, 0.001, -0.0005, -0.001, -0.0005, 0.0025, -0.001, -0.0015, -0.001, 0.0005]

# Use the whiten() function to standardize the data
scaled_data = whiten(rate_cuts)

# Plot original data
plt.plot(rate_cuts, label='original')

# Plot scaled data
plt.plot(scaled_data, label='scaled')

plt.legend()
plt.show()

That is right! Notice how the changes in 
the original data are negligible as compared to the scaled data



################# FIFA 18: Normalize data ######################

# Scale wage and value
fifa['scaled_wage'] = whiten(fifa['eur_wage'])
fifa['scaled_value'] = whiten(fifa['eur_value'])

# Plot the two columns in a scatter plot
fifa.plot(x='scaled_wage', y='scaled_value', kind = 'scatter')
plt.show()

# Check mean and standard deviation of scaled values
print(fifa[['scaled_wage', 'scaled_value']].describe())


That is right! As you can see the scaled values 
have a standard deviation of 1.



############################################### 2. Hierarchical Clustering ###################################


################ Hierarchical clustering: ward method #############

# Import the fcluster and linkage functions
from scipy.cluster.hierarchy import fcluster, linkage

# Use the linkage() function
distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'ward', metric = 'euclidean')

# Assign cluster labels
comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')

# Plot clusters
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = comic_con)
plt.show()



That is correct! Notice the two clusters correspond to
the points of attractions in the figure towards the bottom
(a stage) and the top right (an interesting stall).




############# Hierarchical clustering: single method ###############3

# Import the fcluster and linkage functions
from scipy.cluster.hierarchy import fcluster, linkage

# Use the linkage() function
distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'single', metric = 'euclidean')

# Assign cluster labels
comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')

# Plot clusters
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = comic_con)
plt.show()

That is correct! Notice that in this example,
the clusters formed are not different from the ones created using the ward method.




############### Hierarchical clustering: complete method #################

# Import the fcluster and linkage functions
from scipy.cluster.hierarchy import fcluster, linkage

# Use the linkage() function
distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'complete', metric = 'euclidean')

# Assign cluster labels
comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')

# Plot clusters
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = comic_con)
plt.show()

That is correct! Coincidentally, the clusters formed are not 
different from the ward or single methods. 
Next, let us learn how to visualize clusters.



################## Visualize clusters with matplotlib ##################

# Import the pyplot class
from matplotlib import pyplot as plt

# Define a colors dictionary for clusters
colors = {1:'red', 2:'blue'}

# Plot a scatter plot
comic_con.plot.scatter(x='x_scaled', 
                	   y='y_scaled',
                	   c=comic_con['cluster_labels'].apply(lambda x: colors[x]))
plt.show()


That is correct! The two different clusters are shown in different colors.



###################### Visualize clusters with seaborn ###############

# Import the seaborn module
import seaborn  as sns

# Plot a scatter plot using seaborn
sns.scatterplot(x='x_scaled', 
                y='y_scaled', 
                hue='cluster_labels', 
                data = comic_con)
plt.show()

That is correct! Notice the legend is 
automatically shown when using the hue argument.



################ Create a dendrogram ##############

# Import the dendrogram function
from scipy.cluster.hierarchy import dendrogram

# Create a dendrogram
dn = dendrogram(distance_matrix)

# Display the dendogram
plt.show()

That is correct! Notice the significant difference
between the inter-cluster distances beyond the top two clusters.



########### How many clusters in comic con data? ###########3
Given the dendrogram from the last exercise, how many clusters can you see in the data?

A dendrogram is stored in the variable dn. Use plt.show() to display the dendrogram.



[X]2 clusters

[ ]3 clusters

[ ]4 clusters


You are correct! Notice that the top two clusters are farthest away from each other.




########## Timing run of hierarchical clustering ############333

In earlier exercises of this chapter, you have used the data of 
Comic-Con footfall to create clusters. In this exercise you will 
time how long it takes to run the algorithm on DataCamp's system.

Remember that you can time the execution of small code snippets with:

%timeit sum([1, 3, 2])
The data is stored in a pandas DataFrame, comic_con. x_scaled and 
y_scaled are the column names of the standardized X and Y coordinates 
of people at a given point in time. The timeit module and linkage 
function are already imported

How long does it take to the run the linkage 
function on the comic con data?



[ ]1-5 microseconds

[X]1-5 milliseconds

[ ]1-5 seconds




########### FIFA 18: exploring defenders #########

# Fit the data into a hierarchical clustering algorithm
distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')

# Assign cluster labels to each row of data
fifa['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')

# Display cluster centers of each cluster
print(fifa[['scaled_sliding_tackle', 'scaled_aggression', 'cluster_labels']].groupby('cluster_labels').mean())

# Create a scatter plot through seaborn
sns.scatterplot(x='scaled_sliding_tackle', y='scaled_aggression', hue='cluster_labels', data=fifa)
plt.show()


That is correct! Notice how long it took to run hierarchical
clustering on a few thousand data points. 
In the next chapter, you will explore clusters in data through k-means clustering.



##################################### 3. K-Means Clustering #######################################

################## K-means clustering: first exercise ####################

# Import the kmeans and vq functions
from scipy.cluster.vq import kmeans, vq

# Generate cluster centers
cluster_centers, distortion = kmeans(comic_con[['x_scaled','y_scaled']], 2)

# Assign cluster labels
comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled','y_scaled']], cluster_centers)

# Plot clusters
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = comic_con)
plt.show()


You are correct! Notice that the clusters formed are exactly the 
same as hierarchical clustering that you did in the previous chapter.




############ Elbow method on distinct clusters #################

distortions = []
num_clusters = range(1, 7)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(comic_con[['x_scaled','y_scaled']], i)
    distortions.append(distortion)

# Create a DataFrame with two lists - num_clusters, distortions
elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})

# Creat a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)
plt.xticks(num_clusters)
plt.show()


######## Question
From the elbow plot, how many clusters are there in the data?

Possible Answers

[X]2 clusters

[ ]4 clusters

[ ]6 clusters




distortions = []
num_clusters = range(2, 7)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(uniform_data[['x_scaled','y_scaled']], i)
    distortions.append(distortion)

# Create a DataFrame with two lists - number of clusters and distortions
elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})

# Creat a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data=elbow_plot)
plt.xticks(num_clusters)
plt.show()



### Question
# From the elbow plot, how many clusters are there in the data?


[X]Can not be determined

[ ]3 clusters

[ ]4 clusters



################# Impact of seeds on distinct clusters #####################

# Import random class
from numpy import random

# Initialize seed
random.seed(0)
random.seed([1, 2, 1000])

# Run kmeans clustering
cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)
comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)

# Plot the scatterplot
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = comic_con)
plt.show()


You are correct! Notice that the plots have no
changed after changing the seed as the clusters are well-defined.




################## Uniform clustering patterns #####################

# Import the kmeans and vq functions
from scipy.cluster.vq import kmeans, vq

# Generate cluster centers
cluster_centers, distortion = kmeans(mouse[['x_scaled','y_scaled']], 3)

# Assign cluster labels
mouse['cluster_labels'], distortion_list = vq(mouse[['x_scaled','y_scaled']], cluster_centers)

# Plot clusters
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = mouse)
plt.show()



You are correct! Notice that kmeans is unable to capture 
the three visible clusters clearly, and the two clusters 
towards the top have taken in some points along the boundary. 
This happens due to the underlying assumption in 
kmeans algorithm to minimize distortions which leads 
to clusters that are similar in terms of area.



################# FIFA 18: defenders revisited #############

# Set up a random seed in numpy
random.seed([1000,2000])

# Fit the data into a k-means algorithm
cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)

# Assign cluster labels
fifa['cluster_labels'], _ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)

# Display cluster centers 
print(fifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean())

# Create a scatter plot through seaborn
sns.scatterplot(x='scaled_def', y='scaled_phy', hue='cluster_labels', data=fifa)
plt.show()


You are right! Notice that the seed has an impact
on clustering as the data is uniformly distributed.




################################################### 4. Clustering in Real World ##################################


############# Extract RGB values from image #############

# Import image class of matplotlib
from matplotlib import image as img

# Read batman image and print dimensions
batman_image = img.imread('batman.jpg')
print(batman_image.shape)

# Store RGB values of all pixels in lists r, g and b
for row in batman_image:
    for temp_r, temp_g, temp_b in row:
        r.append(temp_r)
        g.append(temp_g)
        b.append(temp_b)


That is correct! You have successfully 
extracted the RGB values of the image into 
three lists, one for each color channel.



######### How many dominant colors? ###########

distortions = []
num_clusters = range(1, 7)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(batman_df[['scaled_red',       'scaled_blue','scaled_green']], i)
    distortions.append(distortion)

# Create a DataFrame with two lists, num_clusters and distortions
elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})

# Create a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data=elbow_plot)
plt.xticks(num_clusters)
plt.show()


You are right! Notice that there are three distinct
colors present in the image, which is supported by the elbow plot.



############ Display dominant colors ###########

# Get standard deviations of each color
r_std, g_std, b_std = batman_df[['red', 'green', 'blue']].std()

for cluster_center in cluster_centers:
    scaled_r, scaled_g, scaled_b = cluster_center
    # Convert each standardized value to scaled value
    colors.append((
        scaled_r * r_std / 255,
        scaled_g * g_std / 255,
        scaled_b * b_std / 255
    ))

# Display colors of cluster centers
plt.imshow([colors])
plt.show()

That is correct! Notice the three colors resemble
the three that are indicative from visual inspection of the image.



############## TF-IDF of movie plots ##############

# Import TfidfVectorizer class from sklearn
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.75, max_features=50, min_df=0.1, tokenizer=remove_noise)

# Use the .fit_transform() method on the list plots
tfidf_matrix = tfidf_vectorizer.fit_transform(plots)

That is correct! You have successfully created the
sparse matrix. Let us now perform clustering on the matrix.



################### Top terms in movie clusters ##############

num_clusters = 2

# Generate cluster centers through the kmeans function
cluster_centers,_ = kmeans(tfidf_matrix.todense(), num_clusters)

# Generate terms from the tfidf_vectorizer object
terms = tfidf_vectorizer.get_feature_names_out()

for i in range(num_clusters):
    # Sort the terms and print top 3 terms
    center_terms = dict(zip(terms, list(cluster_centers[i])))
    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)
    print(sorted_terms[:3])

You are correct! Notice positive, warm words in the
first cluster and words referring to action in the second cluster.




############# Clustering with many features ##############
## What should you do if you have too many features for clustering?


[ ]Visualize all the features


[X]Reduce features using a technique like Factor Analysis


[ ]Perform hierarchical clustering


You are correct. You should explore steps to reduce the number of features.



######################### Basic checks on clusters ######################

# Print the size of the clusters
print(fifa.groupby('cluster_labels')['ID'].count())

# Print the mean value of wages in each cluster
print(fifa.groupby('cluster_labels')['eur_wage'].mean())



You are correct! In this example, the cluster sizes are not very different,
and there are no significant differences that can be seen in the wages.
Further analysis is required to validate these clusters.



############## FIFA 18: what makes a complete player?n ##############

# Create centroids with kmeans for 2 clusters
cluster_centers,_ = kmeans(fifa[scaled_features], 2)

# Assign cluster labels and print cluster centers
fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)
print(fifa.groupby('cluster_labels')[scaled_features].mean())

# [scaled_features] = ['scaled_pac',
 'scaled_sho',
 'scaled_pas',
 'scaled_dri',
 'scaled_def',
 'scaled_phy']

# Plot cluster centers to visualize clusters
fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')
plt.show()

# Get the name column of first 5 players in each cluster
for cluster in fifa['cluster_labels'].unique():
    print(cluster, fifa[fifa['cluster_labels'] == cluster]['name'].values[:5])



That is correct! The data was sorted before you performed the clustering. 
Notice the top players in each cluster are representative of the overall 
characteristics of the cluster - one of the clusters primarily 
represents attackers, whereas the other represents defenders. 
Surprisingly, a top goalkeeper Manuel Neuer is seen in the attackers group, 
but he is known for going out of the box and participating 
in open play, which are reflected in his FIFA 18 attributes.
