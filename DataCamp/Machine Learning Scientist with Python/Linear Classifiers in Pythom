############# DataCamp ############
######## Linear Classifiers in Pythom ######

##### February 4, 2023 ####




##################### 1. Applying logistic regression and SVM ###############

####### KNN classification #######
from sklearn.neighbors import KNeighborsClassifier

# Create and fit the model
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Predict on the test features, print the results
pred = knn.predict(X_test)[0]
print("Prediction for test example 0:", pred)



#### Comparing models #####

## Possible Answers

[ ]k=1
 
[X]k=5

#### Overfitting #####
# Which of the following situations looks like an example of overfitting?


[ ]Training accuracy 50%, testing accuracy 50%.


[ ]Training accuracy 95%, testing accuracy 95%.


[X]Training accuracy 95%, testing accuracy 50%.


[ ]Training accuracy 50%, testing accuracy 95%.




#### Running LogisticRegression and SVC ####

from sklearn import datasets
digits = datasets.load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)

# Apply logistic regression and print scores
lr = LogisticRegression()
lr.fit(X_train, y_train)
print(lr.score(X_train, y_train))
print(lr.score(X_test, y_test))

# Apply SVM and print scores
svm = SVC()
svm.fit(X_train, y_train)
print(svm.score(X_train, y_train))
print(svm.score(X_test, y_test))


###### Sentiment analysis for movie reviews #######

# Instantiate logistic regression and train
lr = LogisticRegression()
lr.fit(X, y)

# Predict sentiment for a glowing review
review1 = "LOVED IT! This movie was amazing. Top 10 this year."
review1_features = get_features(review1)
print("Review:", review1)
print("Probability of positive review:", lr.predict_proba(review1_features)[0,1])

# Predict sentiment for a poor review
review2 = "Total junk! I'll never watch a film by that director again, no matter how good the reviews."
review2_features = get_features(review2)
print("Review:", review2)
print("Probability of positive review:", lr.predict_proba(review2_features)[0,1])


######## Which decision boundary is linear? #######

[1] 


### Visualizing decision boundaries ####


from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier

# Define the classifiers
classifiers = [LogisticRegression(), LinearSVC(),
               SVC(), KNeighborsClassifier()]

# Fit the classifiers
for c in classifiers:
    c.fit(X, y)

# Plot the classifiers
plot_4_classifiers(X, y, classifiers)
plt.show()





#################  2. Loss functions  #############################3

# How models make predictions
# Which classifiers make predictions based on the sign (positive or negative) of the raw model output?


[ ]Logistic regression only


[ ]Linear SVMs only


[ ]Neither


[X]Both logistic regression and Linear SVMs


########## Changing the model coefficients #########
# Set the coefficients
model.coef_ = np.array([[-1,1]])
model.intercept_ = np.array([-3])

# Plot the data and decision boundary
plot_classifier(X,y,model)

# Print the number of errors
num_err = np.sum(y != model.predict(X))
print("Number of errors:", num_err)

# Great job! As you've been experiencing, the coefficients determine the slope of the boundary and the intercept shifts it.




########### The 0-1 loss ####
# In the figure below, what is the 0-1 loss (number of classification errors) of the classifier?


[ ]0


[ ]1


[X]2


[ ]3


########## Minimizing a loss function ####

# The squared error, summed over training examples
def my_loss(w):
    s = 0
    for i in range(y.size):
        # Get the true and predicted target values for example 'i'
        y_i_true = y[i]
        y_i_pred = w@X[i]
        s = s + (y_i_true-y_i_pred)**2
    return s

# Returns the w that makes my_loss(w) smallest
w_fit = minimize(my_loss, X[0]).x
print(w_fit)

# Compare with scikit-learn's LinearRegression coefficients
lr = LinearRegression(fit_intercept=False).fit(X,y)
print(lr.coef_)


######## Classification loss functions ########
# Which of the four loss functions makes sense for classification?

[ ](1)


[X](2)


[ ](3)


[ ](4)



##### Comparing the logistic and hinge losses ######

# Mathematical functions for logistic and hinge losses
def log_loss(raw_model_output):
   return np.log(1+np.exp(-raw_model_output))
def hinge_loss(raw_model_output):
   return np.maximum(0,1-raw_model_output)

# Create a grid of values and plot
grid = np.linspace(-2,2,1000)
plt.plot(grid, log_loss(grid), label='logistic')
plt.plot(grid, hinge_loss(grid), label='hinge')
plt.legend()
plt.show()


####### Implementing logistic regression #######

# The logistic loss, summed over training examples
def my_loss(w):
    s = 0
    for i in range(y.size):
        raw_model_output = w@X[i]
        s = s + log_loss(raw_model_output * y[i])
    return s

# Returns the w that makes my_loss(w) smallest
w_fit = minimize(my_loss, X[0]).x
print(w_fit)

# Compare with scikit-learn's LogisticRegression
lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)
print(lr.coef_)



########################## 3. Logistic regression #############################

######## Regularized logistic regression ####

# Train and validaton errors initialized as empty list
train_errs = list()
valid_errs = list()

# Loop over values of C_value
for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:
    # Create LogisticRegression object and fit
    lr = LogisticRegression(C=C_value)
    lr.fit(X_train, y_train)
    
    # Evaluate error rates and append to lists
    train_errs.append( 1.0 - lr.score(X_train, y_train) )
    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )
    
# Plot results
plt.semilogx(C_values, train_errs, C_values, valid_errs)
plt.legend(("train", "validation"))
plt.show()


# Congrats! As you can see, too much regularization (small C) 
#doesn't work well - due to underfitting - and too little regularization 
#(large C) doesn't work well either - due to overfitting.



##### Logistic regression and feature selection #####
# Specify L1 regularization
lr = LogisticRegression(solver='liblinear', penalty='l1')

# Instantiate the GridSearchCV object and run the search
searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})
searcher.fit(X_train, y_train)

# Report the best parameters
print("Best CV params", searcher.best_params_)

# Find the number of nonzero coefficients (selected features)
best_lr = searcher.best_estimator_
coefs = best_lr.coef_
print("Total number of features:", coefs.size)
print("Number of selected features:", np.count_nonzero(coefs))

# Great job! As you can see, a whole lot of features were discarded here.




####### Identifying the most positive and negative words


# Get the indices of the sorted cofficients
inds_ascending = np.argsort(lr.coef_.flatten()) 
inds_descending = inds_ascending[::-1]

# Print the most positive words
print("Most positive words: ", end="")
for i in range(5):
    print(vocab[inds_descending[i]], end=", ")
print("\n")

# Print most negative words
print("Most negative words: ", end="")
for i in range(5):
    print(vocab[inds_ascending[i]], end=", ")
print("\n")


######## Getting class probabilities #####
# Which of the following transformations would make sense for 
### transforming the raw model output of a linear classifier into a class probability?

[ ]1

[ ]2

[X]3

[ ]4



##### Regularization and probabilities #####

# Set the regularization strength
model = LogisticRegression(C=0.1)

# Fit and plot
model.fit(X,y)
plot_classifier(X,y,model,proba=True)

# Predict probabilities on training points
prob = model.predict_proba(X)
print("Maximum predicted probability", np.max(prob))

You got it! As you probably noticed, smaller values of C lead to less confident predictions. 
That's because smaller C means more regularization, which in turn means smaller coefficients, 
which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the 
raw model output is squashed through the sigmoid function. That's quite a chain of events!



#### Visualizing easy and difficult examples #####

lr = LogisticRegression()
lr.fit(X,y)

# Get predicted probabilities
proba = lr.predict_proba(X)

# Sort the example indices by their maximum probability
proba_inds = np.argsort(np.max(proba,axis=1))

# Show the most confident (least ambiguous) digit
show_digit(proba_inds[-1], lr)

# Show the least confident (most ambiguous) digit
show_digit(proba_inds[0], lr)

Great job! As you can see, the least confident example looks like a weird 9, 
and the most confident example looks like a very typical 5.





#### Counting the coefficients ####

If you fit a logistic regression model on a classification problem with 3 classes and 100 features, 
how many coefficients would you have, including intercepts?


[ ]101


[ ]103


[ ]301


[X]303



##### Fitting multi-class logistic regression #####

# Fit one-vs-rest logistic regression classifier
lr_ovr = LogisticRegression(multi_class='ovr')
lr_ovr.fit(X_train, y_train)

print("OVR training accuracy:", lr_ovr.score(X_train, y_train))
print("OVR test accuracy    :", lr_ovr.score(X_test, y_test))

# Fit softmax classifier
lr_mn = LogisticRegression(multi_class='multinomial')
lr_mn.fit(X_train, y_train)

print("Softmax training accuracy:", lr_mn.score(X_train, y_train))
print("Softmax test accuracy    :", lr_mn.score(X_test, y_test))

Nice work! As you can see, the accuracies of the two methods are fairly similar on this data set.





###### Visualizing multi-class logistic regression ######

# Print training accuracies
print("Softmax     training accuracy:", lr_mn.score(X_train, y_train))
print("One-vs-rest training accuracy:", lr_ovr.score(X_train, y_train))

# Create the binary classifier (class 1 vs. rest)
lr_class_1 = LogisticRegression(C=100)
lr_class_1.fit(X_train, y_train==1)

# Plot the binary classifier (class 1 vs. rest)
plot_classifier(X_train, y_train==1, lr_class_1)

Nice work! As you can see, the binary classifier incorrectly labels almost all points in class 1
 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective 
component of the one-vs-rest classifier. In general, though, one-vs-rest often works well.



###### One-vs-rest SVM ####

# We'll use SVC instead of LinearSVC from now on
from sklearn.svm import SVC

# Create/plot the binary classifier (class 1 vs. rest)
svm_class_1 = SVC()
svm_class_1.fit(X_train, y_train==1)
plot_classifier(X_train, y_train==1, svm_class_1)


Cool, eh?! The non-linear SVM works fine with one-vs-rest 
on this dataset because it learns to "surround" class 1.




######## Support vector definition #######

# Which of the following is a true statement about support vectors? To help you out, \
# here's the picture of support vectors from the video (top), as well as the hinge loss from Chapter 2 (bottom).


[ ]All support vectors are classified correctly.


[ ]All support vectors are classified incorrectly.


[ ]All correctly classified points are support vectors.


[X]All incorrectly classified points are support vectors.


############ Effect of removing examples ########3

# Train a linear SVM
svm = SVC(kernel="linear")
svm.fit(X, y)
plot_classifier(X, y, svm, lims=(11,15,0,6))

# Make a new data set keeping only the support vectors
print("Number of original examples", len(X))
print("Number of support vectors", len(svm.support_))
X_small = X[svm.support_]
y_small = y[svm.support_]

# Train a new SVM using only the support vectors
svm_small = SVC(kernel="linear")
svm_small.fit(X_small, y_small)
plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))



# Nice! Compare the decision boundaries of the two trained models: 
# are they the same? By the definition of support vectors, they should be!


##### GridSearchCV warm-up ####

# Instantiate an RBF SVM
svm = SVC()

# Instantiate the GridSearchCV object and run the search
parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}
searcher = GridSearchCV(svm, parameters)
searcher.fit(X, y)

# Report the best parameters
print("Best CV params", searcher.best_params_)



Great job! Larger values of gamma are better for training accuracy,
but cross-validation helped us find something different (and better!).


##### Jointly tuning gamma and C with GridSearchCV #######

# Instantiate an RBF SVM
svm = SVC()

# Instantiate the GridSearchCV object and run the search
parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}
searcher = GridSearchCV(svm, parameters)
searcher.fit(X_train, y_train)

# Report the best parameters and the corresponding score
print("Best CV params", searcher.best_params_)
print("Best CV accuracy", searcher.best_score_)

# Report the test accuracy using these best parameters
print("Test accuracy of best grid search hypers:", searcher.score(X_test, y_test))



You got it! Note that the best value of gamma, 0.0001, is different from the value of 0.001 that 
we got in the previous exercise, when we fixed C=1. Hyperparameters can affect each other!



##### An advantage of SVMs #####
# Which of the following is an advantage of SVMs over logistic regression?


[ ]They naturally output meaningful probabilities.


[ ]They can be used with kernels.


[X]They are computationally efficient with kernels.


[ ]They learn sigmoidal decision boundaries.





That's right! Having a limited number of support vectors makes kernel SVMs computationally efficient.



##### An advantage of logistic regression ###########3
# Which of the following is an advantage of logistic regression over SVMs?


[X]It naturally outputs meaningful probabilities.


[ ]It can be used with kernels.


[ ]It is computationally efficient with kernels.

[ ]It learns sigmoidal decision boundaries.


##### Using SGDClassifier ######

# We set random_state=0 for reproducibility 
linear_classifier = SGDClassifier(random_state=0)

# Instantiate the GridSearchCV object and run the search
parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], 
             'loss':["hinge", "log_loss"]}
searcher = GridSearchCV(linear_classifier, parameters, cv=10)
searcher.fit(X_train, y_train)

# Report the best parameters and the corresponding score
print("Best CV params", searcher.best_params_)
print("Best CV accuracy", searcher.best_score_)
print("Test accuracy of best grid search hypers:", searcher.score(X_test, y_test))


One advantage of SGDClassifier is that it's very fast - this would have taken a 
lot longer with LogisticRegression or LinearSVC.
