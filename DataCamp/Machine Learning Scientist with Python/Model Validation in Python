#################################### Model Validation in Python ################################################

########## April 4, 2023 ################

# Modeling steps
# The process of using scikit-learn to create and test models has four steps, and you will use these four steps throughout this course.

# Which of the following is NOT a valid method in the four-step scikit-learn model validation framework?

Possible Answers

[ ].predict()


[ ].fit()


[X].validate()

Correct! Validation is a technique all in its own and is not completed with .validate().
You need to learn a few tools and techniques before you can validate a model.



##################### Seen vs. unseen data ##########################

# The model is fit using X_train and y_train
model.fit(X_train, y_train)

# Create vectors of predictions
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Train/Test Errors
train_error = mae(y_true=y_train, y_pred=train_predictions)
test_error = mae(y_true=y_test, y_pred=test_predictions)

# Print the accuracy for seen and unseen data
print("Model error on seen data: {0:.2f}.".format(train_error))
print("Model error on unseen data: {0:.2f}.".format(test_error))


Excellent. When models perform differently on training and testing data,
you should look to model validation to ensure you have the best performing model.
In the next lesson, you will start building models to validate.


############ Set parameters and fit a model #############

# Set the number of trees
rfr.n_estimators = 100

# Add a maximum depth
rfr.max_depth = 6

# Set the random state
rfr.random_state = 1111

# Fit the model
rfr.fit(X_train, y_train)

Well done! You have updated parameters _after_ the model was initialized.
This approach is helpful when you need to update parameters.
Before making predictions, let's see which candy characteristics were most important to the model.



################### Feature importances ###############


# Fit the model using X and y
rfr.fit(X_train, y_train)

# Print how important each column is to the model
for i, item in enumerate(rfr.feature_importances_):
      # Use i and item to print out the feature importance of each column
    print("{0:s}: {1:.2f}".format(X_train.columns[i], item))

Well done. No surprise here - chocolate _is_ the most important variable. .feature_importances_ is a great way to see which variables were important to your random forest model.



############# Classification predictions #############

# Fit the rfc model. 
rfc.fit(X_train, y_train)

# Create arrays of predictions
classification_predictions = rfc.predict(X_test)
probability_predictions = rfc.predict_proba(X_test)

# Print out count of binary predictions
print(pd.Series(rfc.predict(X_test)).value_counts())

# Print the first value from probability_predictions
print('The first predicted probabilities are: {}'.format(probability_predictions[0]))

Well done! You can see there were 563 observations where Player One was predicted to
win the Tic-Tac-Toe game. Also, note that the predicted_probabilities array contains
lists with only two values because you only have two possible responses (win or lose).
Remember these two methods, as you will use them a lot throughout this course.



################## Reusing model parameters ##################

rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)

# Print the classification model
print(rfc)

# Print the classification model's random state parameter
print('The random state is: {}'.format(rfc.random_state))

# Print all parameters
print('Printing the parameters dictionary: {}'.format(rfc.get_params()))

Well done! Recalling which parameters were used will be helpful going forward.
Model validation and performance rely heavily on which parameters were used,
and there is no way to replicate a model without keeping track of the parameters used!



#################### Random forest classifier ###############


from sklearn.ensemble import RandomForestClassifier

# Create a random forest classifier
rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)

# Fit rfc using X_train and y_train
rfc.fit(X_train, y_train)

# Create predictions on X_test
predictions = rfc.predict(X_test)
print(predictions[0:5])

# Print model accuracy using score() and the testing data
print(rfc.score(X_test, y_test))

That's all the steps! Notice the first five predictions were all 1, indicating that Player One is predicted to win all five of those games. You also see the model accuracy was only 82%.

Let's move on to Chapter 2 and increase our model validation toolbox by learning about splitting datasets, standard accuracy metrics, and the bias-variance tradeoff.




############################################### 2. Validation Basics ############################################


############### Create one holdout set ######################

# Create dummy variables using pandas
X = pd.get_dummies(tic_tac_toe.iloc[:,0:9])
y = tic_tac_toe.iloc[:, 9]


# Create training and testing datasets. Use 10% for the test set
X_train, X_test, y_train, y_test =\
train_test_split(X, y, test_size=0.1, random_state=1111)

Good! Remember, without the holdout set, you cannot truly validate a model. Let's move on to creating two holdout sets.



################ Create two holdout sets #################

# Create temporary training and final testing datasets
X_temp, X_test, y_temp, y_test =\
train_test_split(X, y, test_size=0.2, random_state=1111)

# Create the final training and validation datasets
X_train, X_val, y_train, y_val =\
train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)


Great! You now have training, validation, and testing datasets, but do you know
_when_ you need both validation and testing datasets? Keep going!
The next exercise will help make sure you understand when to use validation datasets.



########### Why use holdout sets
########### It is important to understand when you would use three datasets (training, validation, and testing) instead of two (training and testing). There is no point in creating an additional dataset split if you are not going to use it.

When should you consider using training, validation, and testing datasets?


[ ]When there is a lot of data. Splitting into three sets helps speed up modeling.


[X]When testing parameters, tuning hyper-parameters, or anytime you are frequently evaluating model performance.


[ ]Only when you are running regression and not classification models.


[ ]Only when you are running classification and not regression models




Correct! Anytime we are evaluating model performance repeatedly we need to create training, validation, and testing datasets.




################# Mean absolute error ################


from sklearn.metrics import mean_absolute_error

# Manually calculate the MAE
n = len(predictions)
mae_one = sum(abs(y_test - predictions)) / n
print('With a manual calculation, the error is {}'.format(mae_one))

# Use scikit-learn to calculate the MAE
mae_two = mean_absolute_error(y_test, predictions)
print('Using scikit-learn, the error is {}'.format(mae_two))


Well done! These predictions were about six wins off on average.
This isn't too bad considering NBA teams play 82 games a year.
Let's see how these errors would look if you used the mean squared error instead.



################  Mean squared error #####################

from sklearn.metrics import mean_squared_error

n = len(predictions)
# Finish the manual calculation of the MSE
mse_one = sum((y_test - predictions)**2) / n
print('With a manual calculation, the error is {}'.format(mse_one))

# Use the scikit-learn function to calculate MSE
mse_two = mean_squared_error(y_test, predictions)
print('Using scikit-learn, the error is {}'.format(mse_two))


Good job! If you run any additional models, you will try to beat an
MSE of 49.1, which is the average squared error of using your model.
Although the MSE is not as interpretable as the MAE, it will help us
select a model that has fewer 'large' errors.



################## Performance on data subsets #################

# Find the East conference teams
east_teams = labels == "E"

# Create arrays for the true and predicted values
true_east = y_test[east_teams]
preds_east = predictions[east_teams]

# Print the accuracy metrics
print('The MAE for East teams is {}'.format(
    mae(true_east, preds_east)))

# Print the West accuracy
print('The MAE for West conference is {}'.format(west_error))


Great! It looks like the Western conference predictions were about
two games better on average. Over the past few seasons, the Western
teams have generally won the same number of games as the experts have predicted.
Teams in the East are just not as predictable as those in the West.



################## Confusion matrices ######################

# Calculate and print the accuracy
accuracy = (324  + 491 ) / (953)
print("The overall accuracy is {0: 0.2f}".format(accuracy))

# Calculate and print the precision
precision = (491 ) / (491 + 15)
print("The precision is {0: 0.2f}".format(precision))

# Calculate and print the recall
recall = (491 ) / (123  + 491 )
print("The recall is {0: 0.2f}".format(recall))

Well done! In this case, a true positive is a picture of an actual
broken arm that was also predicted to be broken. Doctors are okay with
a few additional false positives (predicted broken, not actually broken), as long as you don't miss anyone who needs immediate medical attention.



################### Confusion matrices, again ####################

from sklearn.metrics import confusion_matrix

# Create predictions
test_predictions = rfc.predict(X_test)

# Create and print the confusion matrix
cm = confusion_matrix(y_test, test_predictions)
print(cm)

# Print the true positives (actual 1s that were predicted 1s)
print("The number of true positives is: {}".format(cm[1, 1 ]))


Good job! Row 1, column 1 represents the number of actual 1s that were predicted 1s
(the true positives). Always make sure you understand the orientation of
the confusion matrix before you start using it!



###################### Precision vs. recall #########################

from sklearn.metrics import accuracy_score, precision_score, recall_score

test_predictions = rfc.predict(X_test)

# Create precision or recall score based on the metric you imported
score = precision_score(y_test, test_predictions)

# Print the final result
print("The precision value is {0:.2f}".format(score))

Great job! Precision is the correct metric here. Sore-losers can't stand
losing when they are certain they will win! For that reason, our model
needs to be as precise as possible. With a precision of only 79%, you may
need to try some other modeling techniques to improve this score.



################### Error due to under/over-fitting #######################


# Update the rfr model
rfr = RandomForestRegressor(n_estimators=25,
                            random_state=1111,
                            max_features=4)
rfr.fit(X_train, y_train)

# Print the training and testing accuracies 
print('The training error is {0:.2f}'.format(
  mae(y_train, rfr.predict(X_train))))
print('The testing error is {0:.2f}'.format(
  mae(y_test, rfr.predict(X_test))))

Great job! The chart below shows the performance at various max
feature values. Sometimes, setting parameter values can make a huge difference in model performance.



#################### Am I underfitting? ########################


from sklearn.metrics import accuracy_score

test_scores, train_scores = [], []
for i in [1, 2, 3, 4, 5, 10, 20, 50]:
    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)
    rfc.fit(X_train, y_train)
    # Create predictions for the X_train and X_test datasets.
    train_predictions = rfc.predict(X_train)
    test_predictions = rfc.predict(X_test)
    # Append the accuracy score for the test and train predictions.
    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))
    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))
# Print the train and test scores.
print("The training scores were: {}".format(train_scores))
print("The testing scores were: {}".format(test_scores))


Excellent! Notice that with only one tree, both the train and test scores are low.
As you add more trees, both errors improve. Even at 50 trees, this still might not be enough.
Every time you use more trees, you achieve higher accuracy.
At some point though, more trees increase training time, but do not decrease testing error.




###################################################### 3.Cross Validation ###################################################

##################### Two samples ###################

# Create two different samples of 200 observations 
sample1 = tic_tac_toe.sample(200, random_state=1111)
sample2 = tic_tac_toe.sample(200, random_state=1171)

# Print the number of common observations 
print(len([index for index in sample1.index if index in sample2.index]))

# Print the number of observations in the Class column for both samples 
print(sample1['Class'].value_counts())
print(sample2['Class'].value_counts())

Well done! Notice that there are a varying number of positive observations
for both sample test sets. Sometimes creating a single test holdout sample
is not enough to achieve the high levels of model validation you want.
You need to use something more robust.


########################## Potential problems ########################

Potential problems
Which of the following statements are TRUE regarding potential problems with holdout samples:

A: Using different data splitting methods may lead to varying data in the final holdout samples.
B: If you have limited data, your holdout accuracy may be misleading.
C: There are no problems. Creating a single train and test sample is the only way to validate models.
D: You shouldn't use holdout samples with limited data because you are limiting the potential training data.


Possible Answers

[ ]A & D
 

[ ]C & D
 

[X]A & B
 

[ ]A, B, & D



Correct! If our models are not generalizing well or if we have limited data,
we should be careful using a single training/validation split.
You should use the next lesson's topic: cross-validation.


######################### scikit-learn's KFold() ##############################

from sklearn.model_selection import KFold

# Use KFold
kf = KFold(n_splits=5, shuffle=True, random_state=1111)

# Create splits
splits = kf.split(X)

# Print the number of indices
for train_index, val_index in splits:
    print("Number of training indices: %s" % len(train_index))
    print("Number of validation indices: %s" % len(val_index))


Good job! This dataset has 85 rows. You have created five splits - each containing
68 training and 17 validation indices. You can use these indices to complete 5-fold cross-validation.



############################ Using KFold indices #######################################

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

rfc = RandomForestRegressor(n_estimators=25, random_state=1111)

# Access the training and validation indices of splits
for train_index, val_index in splits:
    # Setup the training and validation data
    X_train, y_train = X[train_index], y[train_index]
    X_val, y_val = X[val_index], y[val_index]
    # Fit the random forest model
    rfc.fit(X_train, y_train)
    # Make predictions, and print the accuracy
    predictions = rfc.predict(X_val)
    print("Split accuracy: " + str(mean_squared_error(y_val, predictions)))


Nice work! KFold() is a great method for accessing individual indices when completing cross-validation.
One drawback is needing a for loop to work through the indices though. In the next lesson,
you will look at an automated method for cross-validation using sklearn.



################## scikit-learn's methods ######################

# Instruction 1: Load the cross-validation method
from sklearn.model_selection import cross_val_score

# Instruction 2: Load the random forest regression model
from sklearn.ensemble import RandomForestRegressor

# Instruction 3: Load the mean squared error method
# Instruction 4: Load the function for creating a scorer
from sklearn.metrics import mean_squared_error, make_scorer


Well done! It is easy to see how all of the methods can get mixed up,
but it is important to know the names of the methods you need.
You can always review the scikit-learn documentation should you need any help


###################### Implement cross_val_score() #################################

rfc = RandomForestRegressor(n_estimators=25, random_state=1111)
mse = make_scorer(mean_squared_error)

# Set up cross_val_score
cv = cross_val_score(estimator=rfc,
                     X=X_train,
                     y=y_train,
                     cv=10,
                     scoring=mse)

# Print the mean error
print(cv.mean())


Nice! You now have a baseline score to build on.
If you decide to build additional models or try new techniques,
you should try to get an error lower than 155.56.
Lower errors indicate that your popularity predictions are improving.



###################### When to use LOOCV #######################

# Which of the following are reasons you might NOT run LOOCV on the provided X dataset? The X data has been loaded for you to explore as you see fit.

A: The X dataset has 122,624 data points, which might be computationally expensive and slow.
B: You cannot run LOOCV on classification problems.
C: You want to test different values for 15 different parameters


[ ]A & B

[ ]B & C

[X]A & C

[ ]A

Well done! This many observations will definitely slow things down and could be computationally expensive.
If you don't have time to wait while your computer runs through 1,000 models, you might want to use
5 or 10-fold cross-validation.


#################### Leave-one-out-cross-validation ####################

from sklearn.metrics import mean_absolute_error, make_scorer

# Create scorer
mae_scorer = make_scorer(mean_absolute_error)

rfr = RandomForestRegressor(n_estimators=15, random_state=1111)

# Implement LOOCV
scores = cross_val_score(rfr, X, y, scoring=mae_scorer, cv=85)

# Print the mean and standard deviation
print("The mean of the errors is: %s." % np.mean(scores))
print("The standard deviation of the errors is: %s." % np.std(scores))


Very good! You have come along way with model validation techniques.
The final chapter will wrap up model validation by discussing how to select
the best model and give an introduction to parameter tuning.


####################################################### 4. Selecting the best model with Hyperparameter tuning ############

################ Creating Hyperparameters #############

# Review the parameters of rfr
print(rfr.get_params())

# Maximum Depth
max_depth = [4, 8, 12]

# Minimum samples for a split
min_samples_split = [2, 5, 10]

# Max features 
max_features = [4, 6, 8, 10]

Good job! Hyperparameter tuning requires selecting parameters to tune, as well the possible values these parameters can be set to.



############### Running a model using ranges ###############

from sklearn.ensemble import RandomForestRegressor

# Fill in rfr using your variables
rfr = RandomForestRegressor(
    n_estimators=100,
    max_depth=random.choice(max_depth),
    min_samples_split=random.choice(min_samples_split),
    max_features=random.choice(max_features))

# Print out the parameters
print(rfr.get_params())


Good job! Notice that min_samples_split was randomly set to 2.
Since you specified a random state, min_samples_split will
always be set to 2 if you only run this model one time.



################# Preparing for RandomizedSearch ####################

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import make_scorer, mean_squared_error

# Finish the dictionary by adding the max_depth parameter
param_dist = {"max_depth": [2,4,6,8],
              "max_features": [2, 4, 6, 8, 10],
              "min_samples_split": [2, 4, 8, 16]}

# Create a random forest regression model
rfr = RandomForestRegressor(n_estimators=10, random_state=1111)

# Create a scorer to use (use the mean squared error)
scorer = make_scorer(mean_squared_error)



Well done! To use RandomizedSearchCV(), you need a distribution dictionary, an estimator,
and a scorer—once you've got these, you can run a random search to find the best parameters for your model.



################# Implementing RandomizedSearchCV #####################

# Import the method for random search
from sklearn.model_selection import RandomizedSearchCV

# Build a random search using param_dist, rfr, and scorer
random_search =\
    RandomizedSearchCV(
        estimator=rfr,
        param_distributions=param_dist,
        n_iter=10,
        cv=5,
        scoring=scorer)


Nice! Although it takes a lot of steps, hyperparameter tuning with random search is
well worth it and can improve the accuracy of your models. Plus, you are already
using cross-validation to validate your best model.


##################### Best classification accuracy ####################

#You are in a competition at work to build the best model for predicting the winner of a Tic-Tac-Toe game. You already ran a random search and saved the results of the most accurate model to rs.

#Which parameter set produces the best classification accuracy?


[ ]{'max_depth': 8, 'min_samples_split': 4, 'n_estimators': 10

[ ]{'max_depth': 2, 'min_samples_split': 4, 'n_estimators': 10}

[X]{'max_depth': 12, 'min_samples_split': 4, 'n_estimators': 20}

[ ]{'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 50}


Perfect! These parameters do produce the best testing accuracy.
Good job! Remember, to reuse this model you can use rs.best_estimator_.


################# Selecting the best precision model #######################

from sklearn.metrics import precision_score, make_scorer

# Create a precision scorer
precision = make_scorer(precision_score)
# Finalize the random search
rs = RandomizedSearchCV(
  estimator=rfc, param_distributions=param_dist,
  scoring = precision,
  cv=5, n_iter=10, random_state=1111)
rs.fit(X, y)

# print the mean test scores:
print('The accuracy for each run was: {}.'.format(rs.cv_results_['mean_test_score']))
# print the best model score:
print('The best accuracy for a single model was: {}'.format(rs.best_score_))

Wow - Your model's precision was 93%! The best model accurately predicts a winning game 93% of the time.
If you look at the mean test scores, you can tell some of the other parameter sets did really poorly.
Also, since you used cross-validation, you can be confident in your predictions. Well done!



