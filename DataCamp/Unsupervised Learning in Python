########### Data camp ############
################## Unsupervised Learning in Python ###############
#### February, 2023 #########


####################### 1. Clustering for dataset exploration #################

##### How many clusters? ###########


[ ]2

[X]3

[ ]300


###### Clustering 2D points ######

# Import KMeans
from sklearn.cluster import KMeans

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(points)

# Determine the cluster labels of new_points: labels
labels = model.predict(new_points)

# Print cluster labels of new_points
print(labels)

Great work! You've successfully performed k-Means 
clustering and predicted the labels of new points. 
But it is not easy to inspect the clustering by just looking at the printed labels. 
A visualization would be far more useful. In the next exercise, 
you'll inspect your clustering with a scatter plot!



################# Inspect your clustering ##############

# Import pyplot
from matplotlib import pyplot as plt

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()


Fantastic! The clustering looks great! But how can you be sure 
that 3 clusters is the correct choice? In other words, how can 
you evaluate the quality of a clustering? Tune into the next 
video in which Ben will explain how to evaluate a clustering!


########### How many clusters of grain? #############

ks = range(1, 6)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(samples)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

Excellent job! The inertia decreases very slowly 
from 3 clusters to 4, so it looks like 3 clusters 
would be a good choice for this data.



######### Evaluating the grain clustering ############

# Create a KMeans model with 3 clusters: model
model = KMeans(n_clusters=3)

# Use fit_predict to fit model and obtain cluster labels: labels
labels = model.fit_predict(samples)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['varieties'])

# Display ct
print(ct)


Great work! The cross-tabulation shows that the 3 varieties
 of grain separate really well into 3 clusters. 
But depending on the type of data you are working with, 
the clustering may not always be this good. Is there anything 
you can do in such situations to improve your clustering?
You'll find out in the next video!



######### Scaling fish data for clustering ##########

# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)

Great work! Now that you've built the pipeline, 
you'll use it in the next exercise to cluster the fish by their measurements.




############ Clustering the fish data ##########

# Import pandas
import pandas as pd

# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({'labels': labels, 'species':species})

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['species'])

# Display ct
print(ct)

Excellent! It looks like the fish data separates really well into 4 clusters!




########### Clustering stocks using KMeans ##########

# Import Normalizer
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)

Great work - you're really getting the hang of this. 
Now that your pipeline has been set up, you can find 
out which stocks move together in the next exercise!



########## Which stocks move together? #########

# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({'labels': labels, 'companies': companies})

# Display df sorted by cluster label
print(df.sort_values('labels'))

Fantastic job - you have completed Chapter 1! 
Take a look at the clusters. Are you surprised by any of the results? 
In the next chapter, you'll learn about how to communicate 
results such as this through visualizations.




####################### 2. Visualization with hierarchical clustering and t-SNE #######################

######## How many merges? ##########
If there are 5 data samples, how many merge operations will occur in a hierarchical clustering? (To help answer this question, think back to the video, in which Ben walked through an example of hierarchical clustering using 6 countries.)


[X]4 merges.


[ ]3 merges.

[ ]This can't be known in advance.


Well done! With 5 data samples, there would be 4 merge operations, 
and with 6 data samples, there would be 5 merges, and so on.



######## Hierarchical clustering of the grain data#########

# Perform the necessary imports
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

# Calculate the linkage: mergings
mergings = linkage(samples, method='complete')

# Plot the dendrogram, using varieties as labels
dendrogram(mergings,
           labels=varieties, 
           leaf_rotation=90,
           leaf_font_size=6
)
plt.show()

Superb! Dendrograms are a great way to illustrate
 the arrangement of the clusters produced by hierarchical clustering.



######## Hierarchies of stocks ########



# Import normalize
from sklearn.preprocessing import normalize

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements , method='complete')

# Plot the dendrogram
dendrogram(mergings,
           labels=companies, 
           leaf_rotation=90,
           leaf_font_size=6
)
plt.show()


Great work! You can produce great visualizations such 
as this with hierarchical clustering, but it can be
 used for more than just visualizations. 
You'll find out more about this in the next video!



########### Which clusters are closest? ####
#In the video, you learned that the linkage method defines how the distance between clusters is measured. In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters.

#Consider the three clusters in the diagram. Which of the following statements are true?
#A. In single linkage, Cluster 3 is the closest cluster to Cluster 2.

#B. In complete linkage, Cluster 1 is the closest cluster to Cluster 2.



[ ]Neither A nor B.


[ ]A only.

[X]Both A and B.



########## Different linkage, different hierarchical clustering! ########


# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram

# Calculate the linkage: mergings
mergings = linkage(samples, method='single')

# Plot the dendrogram
dendrogram(mergings,
           labels=country_names, 
           leaf_rotation=90,
           leaf_font_size=6
)
plt.show()


Great work! As you can see, performing single linkage 
hierarchical clustering produces a different dendrogram!



########## Intermediate clusterings ############
# Displayed on the right is the dendrogram for 
the hierarchical clustering of the grain samples 
that you computed earlier. If the hierarchical clustering
 were stopped at height 6 on the dendrogram, 
how many clusters would there be?



[ ]1.

[X]3.

[ ]As many as there were at the beginning.




############ Extracting the cluster labels #######

# Perform the necessary imports
import pandas as pd
from scipy.cluster.hierarchy import fcluster

# Use fcluster to extract labels: labels
labels = fcluster(mergings, 6, criterion='distance')

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['varieties'])

# Display ct
print(ct)

Fantastic - you've now mastered the fundamentals of 
k-Means and agglomerative hierarchical clustering. 
Next, you'll learn about t-SNE, which is a powerful 
tool for visualizing high dimensional data.



############ t-SNE visualization of grain dataset ###########

# Import TSNE
from sklearn.manifold import TSNE

# Create a TSNE instance: model
model = TSNE(learning_rate=200)

# Apply fit_transform to samples: tsne_features
tsne_features = model.fit_transform(samples)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1st feature: ys
ys = tsne_features[:,1]

# Scatter plot, coloring by variety_numbers
plt.scatter(xs, ys, c=variety_numbers)
plt.show()


Excellent! As you can see, the t-SNE visualization 
manages to separate the 3 varieties of grain samples. 
But how will it perform on the stock data? 
You'll find out in the next exercise!




######## A t-SNE map of the stock market #########

# Import TSNE
from sklearn.manifold import TSNE

# Create a TSNE instance: model
model = TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
plt.scatter(xs, ys, alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
plt.show()


Fantastic! It's visualizations such as this that make t-SNE such a 
powerful tool for extracting quick insights from high dimensional data.






############################ 3. Decorrelating your data and dimension reduction #######################################

######## Correlated data in nature ########

# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Assign the 0th column of grains: width
width = grains[:,0]

# Assign the 1st column of grains: length
length = grains[:,1]

# Scatter plot width vs length
plt.scatter(width, length)
plt.axis('equal')
plt.show()

# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width, length)

# Display the correlation
print(correlation)


Great work! As you would expect, 
the width and length of the grain 
samples are highly correlated.




############# Decorrelating the grain measurements with PCA ############

# Import PCA
from sklearn.decomposition import PCA 

# Create PCA instance: model
model = PCA()

# Apply the fit_transform method of model to grains: pca_features
pca_features = model.fit_transform(grains)

# Assign 0th column of pca_features: xs
xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis('equal')
plt.show()

# Calculate the Pearson correlation of xs and ys
correlation, pvalue = pearsonr(xs, ys)

# Display the correlation
print(correlation)

Excellent! You've successfully decorrelated the grain measurements with PCA!



########### Principal components #############3
# On the right are three scatter plots of the same point cloud. Each scatter plot shows a different set of axes (in red). In which of the plots could the axes represent the principal components of the point cloud?

# Recall that the principal components are the directions along which the the data varies.


[ ]None of them.

[X]Both plot 1 and plot 3.

[ ]Plot 2.




Well done! You've correctly inferred that the principal 
components have to align with the axes of the point cloud. 
This happens in both plot 1 and plot 3.




######### The first principal component ##############

# Make a scatter plot of the untransformed points
plt.scatter(grains[:,0], grains[:,1])

# Create a PCA instance: model
model = PCA()

# Fit model to points
model.fit(grains)

# Get the mean of the grain samples: mean
mean = model.mean_

# Get the first principal component: first_pc
first_pc = model.components_[0,:]

# Plot first_pc as an arrow, starting at mean
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)

# Keep axes on same scale
plt.axis('equal')
plt.show()




############### Variance of the PCA features ############

# Perform the necessary imports
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
plt.show()


Great work! It looks like PCA features 0 and 1 have significant variance.



######### Intrinsic dimension of the fish data #######
# In the previous exercise, you plotted the variance of the 
PCA features of the fish measurements. Looking again at your plot, 
what do you think would be a reasonable choice for the 
"intrinsic dimension" of the fish measurements? Recall that the
intrinsic dimension is the number of PCA features with significant variance.



[ ]1

[X]2

[ ]5

Great job! Since PCA features 0 and 1 have significant variance, 
the intrinsic dimension of this dataset appears to be 2.



############# Dimension reduction of the fish measurements #########

# Import PCA
from sklearn.decomposition import PCA

# Create a PCA model with 2 components: pca
pca = PCA(n_components=2)

# Fit the PCA instance to the scaled samples
pca.fit(scaled_samples)

# Transform the scaled samples: pca_features
pca_features = pca.transform(scaled_samples)

# Print the shape of pca_features
print(pca_features.shape)

Superb! You've successfully reduced the dimensionality from 6 to 2.


##########  A tf-idf word-frequency array #######


# Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer() 

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)


Great work! You'll now move to clustering Wikipedia articles!


########### Clustering Wikipedia part I ############

# Perform the necessary imports
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline

# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)


Excellent! Now that you have set up your pipeline, 
you will use it in the next exercise to cluster the articles.


################## Clustering Wikipedia part II ############

# Import pandas
import pandas as pd

# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)

# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
print(df.sort_values('label'))


Fantastic! Take a look at the cluster labels and see if you can identify any patterns!



################################ 4. Discovering interpretable features #####################

########## Non-negative data ###########3
# Which of the following 2-dimensional arrays are examples of non-negative data?

1. A tf-idf word-frequency array.
2. An array daily stock market price movements (up and down), 
where each row represents a company.
3. An array where rows are customers, columns are products and 
entries are 0 or 1, indicating whether a customer has purchased a product.

[ ]1 only


[ ]2 and 3


[X]1 and 3



Well done! Stock prices can go down as well as up, 
so an array of daily stock market price movements is not an example of non-negative data.


###################### NMF applied to Wikipedia articles ###############

# Import NMF
from sklearn.decomposition import NMF 

# Create an NMF instance: model
model = NMF(n_components=6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(nmf_features.round(2))




##################### NMF features of the Wikipedia articles ##############

# Import pandas
import pandas as pd

# Create a pandas DataFrame: df
df = pd.DataFrame(nmf_features, index=titles)

# Print the row for 'Anne Hathaway'
print(df.loc['Anne Hathaway'])

# Print the row for 'Denzel Washington'
print(df.loc['Denzel Washington'])


Great work! Notice that for both actors, 
the NMF feature 3 has by far the highest value. 
This means that both articles are reconstructed 
using mainly the 3rd NMF component. 
In the next video, you'll see why: 
NMF components represent topics (for instance, acting!).



########### NMF reconstructs samples ##########
In this exercise, you'll check your understanding of how 
NMF reconstructs samples from its components using the NMF 
feature values. On the right are the components of an NMF model. 
If the NMF feature values of a sample are [2, 1], then which 
of the following is most likely to represent the original sample? 
A pen and paper will help here! You have to apply the same technique 
Ben used in the video to reconstruct the sample [0.1203 0.1764 0.3195 0.141].



[X][2.2, 1.1, 2.1].

[ ][0.5, 1.6, 3.1].

[ ][-4.0, 1.0, -2.0].



################## NMF learns topics of documents #############

# Import pandas
import pandas as pd

# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=words)

# Print the shape of the DataFrame
print(components_df.shape)

# Select row 3: component
component = components_df.iloc[3]

# Print result of nlargest
print(component.nlargest())


Great work! Take a moment to recognise the topics 
that the articles about Anne Hathaway and 
Denzel Washington have in common!



###### Explore the LED digits dataset #####

# Import pyplot
from matplotlib import pyplot as plt

# Select the 0th row: digit
digit = samples[0,:]

# Print digit
print(digit)

# Reshape digit to a 13x8 array: bitmap
bitmap = digit.reshape(13, 8)

# Print bitmap
print(bitmap)

# Use plt.imshow to display bitmap
plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.colorbar()
plt.show()

Excellent job! You'll explore this dataset further in 
the next exercise and see for yourself how NMF can learn the parts of images.



############# NMF learns the parts of images ##############

# Import NMF
from sklearn.decomposition import NMF 

# Create an NMF model: model
model = NMF(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
    show_as_image(component)

# Select the 0th row of features: digit_features
digit_features = features[0,:]

# Print digit_features
print(digit_features)

Great work! Take a moment to look through the plots and notice how 
NMF has expressed the digit as a sum of the components!





############ PCA doesn't learn parts ###########3


# Import PCA
from sklearn.decomposition import PCA 

# Create a PCA instance: model
model = PCA(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
    show_as_image(component)


Great work! Notice that the components of PCA do
not represent meaningful parts of images of LED digits!



############## Which articles are similar to 'Cristiano Ronaldo'? ###############

# Perform the necessary imports
import pandas as pd
from sklearn.preprocessing import normalize

# Normalize the NMF features: norm_features
norm_features = normalize(nmf_features)

# Create a DataFrame: df
df = pd.DataFrame(norm_features, titles)

# Select the row corresponding to 'Cristiano Ronaldo': article
article = df.loc['Cristiano Ronaldo']

# Compute the dot products: similarities
similarities = df.dot(article)

# Display those with the largest cosine similarity
print(similarities.nlargest())


Great work - although you may need to know a little about football
(or soccer, depending on where you're from!) to be able to 
evaluate for yourself the quality of the computed similarities!


########### Recommend musical artists part I #############

# Perform the necessary imports
from sklearn.decomposition import NMF
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline

# Create a MaxAbsScaler: scaler
scaler = MaxAbsScaler()

# Create an NMF model: nmf
nmf = NMF(n_components=20)

# Create a Normalizer: normalizer
normalizer = Normalizer()

# Create a pipeline: pipeline
pipeline = make_pipeline(scaler, nmf, normalizer)

# Apply fit_transform to artists: norm_features
norm_features = pipeline.fit_transform(artists)


Excellent work - now that you've computed the normalized NMF features, 
you'll use them in the next exercise to recommend musical artists!



############# Recommend musical artists part II ############

# Import pandas
import pandas as pd

# Create a DataFrame: df
df = pd.DataFrame(norm_features, artist_names)

# Select row of 'Bruce Springsteen': artist
artist = df.loc['Bruce Springsteen']

# Compute cosine similarities: similarities
similarities = df.dot(artist)

# Display those with highest cosine similarity
print(similarities.nlargest())
